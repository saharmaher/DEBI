{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":98994,"databundleVersionId":11802362,"sourceType":"competition"},{"sourceId":11628908,"sourceType":"datasetVersion","datasetId":7295955}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:25:42.663391Z","iopub.execute_input":"2025-04-30T16:25:42.663595Z","iopub.status.idle":"2025-04-30T16:25:44.428620Z","shell.execute_reply.started":"2025-04-30T16:25:42.663572Z","shell.execute_reply":"2025-04-30T16:25:44.428031Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-arabic-tweets-debi-intake-2/sample_submission.csv\n/kaggle/input/nlp-arabic-tweets-debi-intake-2/train.csv\n/kaggle/input/nlp-arabic-tweets-debi-intake-2/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\n# If there's a GPU available...\nif torch.cuda.is_available():    \n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n    !nvidia-smi\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:25:44.429804Z","iopub.execute_input":"2025-04-30T16:25:44.430120Z","iopub.status.idle":"2025-04-30T16:25:49.551807Z","shell.execute_reply.started":"2025-04-30T16:25:44.430101Z","shell.execute_reply":"2025-04-30T16:25:49.550852Z"}},"outputs":[{"name":"stdout","text":"There are 2 GPU(s) available.\nWe will use the GPU: Tesla T4\nWed Apr 30 16:25:49 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   43C    P8             10W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   45C    P8             10W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Dependencies Installation","metadata":{}},{"cell_type":"code","source":"!pip install gdown\n!pip install pyarabic\n!pip install farasapy\n!pip install emoji\n!pip install transformers\n!git clone https://github.com/aub-mind/arabert.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:26:31.628303Z","iopub.execute_input":"2025-04-30T16:26:31.628611Z","iopub.status.idle":"2025-04-30T16:26:48.781742Z","shell.execute_reply.started":"2025-04-30T16:26:31.628583Z","shell.execute_reply":"2025-04-30T16:26:48.780981Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\nRequirement already satisfied: pyarabic in /usr/local/lib/python3.11/dist-packages (0.6.15)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from pyarabic) (1.17.0)\nCollecting farasapy\n  Downloading farasapy-0.0.14-py3-none-any.whl.metadata (8.9 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from farasapy) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from farasapy) (4.67.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (2025.1.31)\nDownloading farasapy-0.0.14-py3-none-any.whl (11 kB)\nInstalling collected packages: farasapy\nSuccessfully installed farasapy-0.0.14\nRequirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nCloning into 'arabert'...\nremote: Enumerating objects: 600, done.\u001b[K\nremote: Counting objects: 100% (65/65), done.\u001b[K\nremote: Compressing objects: 100% (33/33), done.\u001b[K\nremote: Total 600 (delta 38), reused 45 (delta 30), pack-reused 535 (from 1)\u001b[K\nReceiving objects: 100% (600/600), 9.14 MiB | 36.57 MiB/s, done.\nResolving deltas: 100% (339/339), done.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"Data_set = pd.read_csv(\"/kaggle/input/nlp-arabic-tweets-debi-intake-2/train.csv\")\nData_set","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:27:38.129623Z","iopub.execute_input":"2025-04-30T16:27:38.130331Z","iopub.status.idle":"2025-04-30T16:27:38.189897Z","shell.execute_reply.started":"2025-04-30T16:27:38.130300Z","shell.execute_reply":"2025-04-30T16:27:38.189301Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                  tweet class\n0     ' #علمتني_الحياه أن الذين يعيشون على الأرض ليس...   pos\n1     ' #ميري_كرسمس كل سنة وانتم طيبين http://t.co/n...   pos\n2                             ' و انتهى مشوار الخواجة '   neg\n3                      ' مش عارف ابتدى مذاكره منين :/ '   neg\n4     ' @mskhafagi  إختصروا الطريق بدلا من إختيار ال...   neg\n...                                                 ...   ...\n2054  ' @wasfa_N الجمال مبيحتاح اي مكياج لناعم وله خ...   neu\n2055  ' @TheMurexDor نتمني وجود الفنانة رنا سماحة اف...   neu\n2056  ' ولد الهدى فالكائنات ضياء .. وفم الزمان تبسم ...   pos\n2057  ' @mohamed71944156 @samarroshdy1 انت متناقض جد...   neg\n2058  ' منطقة السيدة زينب ليلة المولد @ مسجد السيدة ...   neu\n\n[2059 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>' #علمتني_الحياه أن الذين يعيشون على الأرض ليس...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>' #ميري_كرسمس كل سنة وانتم طيبين http://t.co/n...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>' و انتهى مشوار الخواجة '</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>' مش عارف ابتدى مذاكره منين :/ '</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>' @mskhafagi  إختصروا الطريق بدلا من إختيار ال...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>' @wasfa_N الجمال مبيحتاح اي مكياج لناعم وله خ...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>' @TheMurexDor نتمني وجود الفنانة رنا سماحة اف...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>' ولد الهدى فالكائنات ضياء .. وفم الزمان تبسم ...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>' @mohamed71944156 @samarroshdy1 انت متناقض جد...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>' منطقة السيدة زينب ليلة المولد @ مسجد السيدة ...</td>\n      <td>neu</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"### Dataset Preparation","metadata":{}},{"cell_type":"code","source":"arabic_stop_words=[]\nwith open ('/kaggle/input/arabic-stop-words/list.txt',encoding='utf-8') as f :\n    for i in f.readlines() :\n        arabic_stop_words.append(i)\n        arabic_stop_words[-1]=arabic_stop_words[-1][:-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:32:35.402830Z","iopub.execute_input":"2025-04-30T16:32:35.403588Z","iopub.status.idle":"2025-04-30T16:32:35.413448Z","shell.execute_reply.started":"2025-04-30T16:32:35.403561Z","shell.execute_reply":"2025-04-30T16:32:35.412853Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport string,emoji, re\nimport pyarabic.araby as ar\nimport functools, operator\nimport logging\nlogging.basicConfig(level=logging.WARNING)\nlogger = logging.getLogger(__name__)\n\ndef get_emoji_regexp():\n    # Sort emoji by length to make sure multi-character emojis are matched first\n    emojis = sorted(emoji.EMOJI_DATA, key=len, reverse=True)\n    pattern = u'(' + u'|'.join(re.escape(u) for u in emojis) + u')'\n    return re.compile(pattern)\n\ndef data_cleaning (text):\n    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r\"http\\S+\", \"\", text)\n    text = re.sub(r\"https\\S+\", \"\", text)\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(\"(\\s\\d+)\",\"\",text)\n    text = re.sub(r\"$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$\", \"\", text)\n    text = re.sub(\"\\d+\", \" \", text)\n    text = ar.strip_tashkeel(text)\n    text = ar.strip_tatweel(text)\n    text = text.replace(\"#\", \" \");\n    text = text.replace(\"@\", \" \");\n    text = text.replace(\"_\", \" \");\n    \n    # Remove arabic signs\n    text = text[0:2] + ''.join([text[i] for i in range(2, len(text)) if text[i]!=text[i-1] or text[i]!=text[i-2]])\n    text =  re.sub(r'([@A-Za-z0-9_ـــــــــــــ]+)|[^\\w\\s]|#|http\\S+', '', text)\n    text =  '' if text in arabic_stop_words else text\n    from nltk.stem.isri import ISRIStemmer\n    text=ISRIStemmer().stem(text)\n    \n    translator = str.maketrans('', '', string.punctuation)\n    text = text.translate(translator)\n    em = text\n    em_split_emoji = get_emoji_regexp().split(em)\n    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n    em_split = functools.reduce(operator.concat, em_split_whitespace)\n    text = \" \".join(em_split)\n    text = re.sub(r'(.)\\1+', r'\\1', text)\n    \n    text = text.replace(\"آ\", \"ا\")\n    text = text.replace(\"إ\", \"ا\")\n    text = text.replace(\"أ\", \"ا\")\n    text = text.replace(\"ؤ\", \"و\")\n    text = text.replace(\"ئ\", \"ي\")\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:33:14.180543Z","iopub.execute_input":"2025-04-30T16:33:14.181067Z","iopub.status.idle":"2025-04-30T16:33:14.369554Z","shell.execute_reply.started":"2025-04-30T16:33:14.181035Z","shell.execute_reply":"2025-04-30T16:33:14.368790Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"Data_set['tweet']=Data_set['tweet'].apply(lambda x: data_cleaning(x))\nData_set","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:33:37.087259Z","iopub.execute_input":"2025-04-30T16:33:37.087792Z","iopub.status.idle":"2025-04-30T16:33:45.738403Z","shell.execute_reply.started":"2025-04-30T16:33:37.087767Z","shell.execute_reply":"2025-04-30T16:33:45.737756Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                  tweet class\n0     علمتني الحياه ان الذين يعيشون على الارض ليسوا ...   pos\n1                         ميري كرسمس كل سنة وانتم طيبين   pos\n2                                 و انتهى مشوار الخواجة   neg\n3                             مش عارف ابتدى مذاكره منين   neg\n4     اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...   neg\n...                                                 ...   ...\n2054  الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...   neu\n2055  نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...   neu\n2056  ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...   pos\n2057                             انت متناقض جدا يا صلاح   neg\n2058     منطقة السيدة زينب ليلة المولد مسجد السيدة زينب   neu\n\n[2059 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>علمتني الحياه ان الذين يعيشون على الارض ليسوا ...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ميري كرسمس كل سنة وانتم طيبين</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>و انتهى مشوار الخواجة</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>مش عارف ابتدى مذاكره منين</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>انت متناقض جدا يا صلاح</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>منطقة السيدة زينب ليلة المولد مسجد السيدة زينب</td>\n      <td>neu</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"### Dataset Tokonization","metadata":{}},{"cell_type":"code","source":"from arabert.preprocess import ArabertPreprocessor\n\nmodel_name = \"UBC-NLP/MARBERT\"\ndf = Data_set\narabert_prep = ArabertPreprocessor(model_name=model_name)\ndf['tweet']=Data_set['tweet'].apply(lambda x: arabert_prep.preprocess(x))\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:34:05.451522Z","iopub.execute_input":"2025-04-30T16:34:05.452439Z","iopub.status.idle":"2025-04-30T16:34:05.605075Z","shell.execute_reply.started":"2025-04-30T16:34:05.452402Z","shell.execute_reply":"2025-04-30T16:34:05.604459Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                  tweet class\n0     علمتني الحياه ان الذين يعيشون على الارض ليسوا ...   pos\n1                         ميري كرسمس كل سنة وانتم طيبين   pos\n2                                 و انتهى مشوار الخواجة   neg\n3                             مش عارف ابتدى مذاكره منين   neg\n4     اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...   neg\n...                                                 ...   ...\n2054  الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...   neu\n2055  نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...   neu\n2056  ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...   pos\n2057                             انت متناقض جدا يا صلاح   neg\n2058     منطقة السيدة زينب ليلة المولد مسجد السيدة زينب   neu\n\n[2059 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>علمتني الحياه ان الذين يعيشون على الارض ليسوا ...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ميري كرسمس كل سنة وانتم طيبين</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>و انتهى مشوار الخواجة</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>مش عارف ابتدى مذاكره منين</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>انت متناقض جدا يا صلاح</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>منطقة السيدة زينب ليلة المولد مسجد السيدة زينب</td>\n      <td>neu</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"### Label Encoding","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nlable_encoder = preprocessing.LabelEncoder()\n\nencoded_labels=lable_encoder.fit_transform(Data_set[\"class\"])\ndf['class']=encoded_labels\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:35:41.728878Z","iopub.execute_input":"2025-04-30T16:35:41.729429Z","iopub.status.idle":"2025-04-30T16:35:41.743466Z","shell.execute_reply.started":"2025-04-30T16:35:41.729404Z","shell.execute_reply":"2025-04-30T16:35:41.742682Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                  tweet  class\n0     علمتني الحياه ان الذين يعيشون على الارض ليسوا ...      2\n1                         ميري كرسمس كل سنة وانتم طيبين      2\n2                                 و انتهى مشوار الخواجة      0\n3                             مش عارف ابتدى مذاكره منين      0\n4     اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...      0\n...                                                 ...    ...\n2054  الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...      1\n2055  نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...      1\n2056  ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...      2\n2057                             انت متناقض جدا يا صلاح      0\n2058     منطقة السيدة زينب ليلة المولد مسجد السيدة زينب      1\n\n[2059 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>علمتني الحياه ان الذين يعيشون على الارض ليسوا ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ميري كرسمس كل سنة وانتم طيبين</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>و انتهى مشوار الخواجة</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>مش عارف ابتدى مذاكره منين</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>انت متناقض جدا يا صلاح</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>منطقة السيدة زينب ليلة المولد مسجد السيدة زينب</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"seed = 42\nfrom sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation=train_test_split(df['tweet'], df['class'], test_size=0.2, random_state=seed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:36:02.856084Z","iopub.execute_input":"2025-04-30T16:36:02.856385Z","iopub.status.idle":"2025-04-30T16:36:02.863042Z","shell.execute_reply.started":"2025-04-30T16:36:02.856363Z","shell.execute_reply":"2025-04-30T16:36:02.862265Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### TF-IDF Embedding","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ndef tfidf_ngram(n_gram,X_train,X_val):\n    vectorizer = TfidfVectorizer(ngram_range=(n_gram,n_gram))\n    x_train_vec = vectorizer.fit_transform(X_train)\n    x_test_vec = vectorizer.transform(X_val)\n    return x_train_vec,x_test_vec\n# Applying tfidf with 1-gram, and 2-gram\ntfidf_1g_transformation_train,tfidf_1g_transformation_validation= tfidf_ngram(1,X_train,X_validation)\ntfidf_2g_transformation_train,tfidf_2g_transformation_validation= tfidf_ngram(2,X_train,X_validation)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:36:33.705332Z","iopub.execute_input":"2025-04-30T16:36:33.705929Z","iopub.status.idle":"2025-04-30T16:36:33.843146Z","shell.execute_reply.started":"2025-04-30T16:36:33.705905Z","shell.execute_reply":"2025-04-30T16:36:33.842503Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Models and embeddings setup\nmodels = [\n    SVC(),\n    KNeighborsClassifier(),\n    XGBClassifier(),\n    RandomForestClassifier(),\n    DecisionTreeClassifier(),\n    LogisticRegression(),\n    MultinomialNB()\n]\n\ntext_embedding = {\n    'TF_IDF 1_gram': (tfidf_1g_transformation_train, tfidf_1g_transformation_validation),\n    'TF_IDF 2_gram': (tfidf_2g_transformation_train, tfidf_2g_transformation_validation)\n}\n\n# Store results\nhighest_test_accuracy = 0\nchampion_model_name = ''\nchampion_model = None\nchampion_embedding = ''\nresults_dict = {\n    'Model Name': [],\n    'Embedding type': [],\n    'Training Accuracy': [],\n    'Testing Accuracy': []\n}\n\n# Evaluate models\nfor model in models:\n    for embedding_vector in text_embedding:\n        train = text_embedding[embedding_vector][0]\n        test = text_embedding[embedding_vector][1]\n\n        model.fit(train, y_train)\n        train_acc = model.score(train, y_train)\n        test_acc = model.score(test, y_validation)\n\n        model_name = type(model).__name__.replace(\"Classifier\", \"\")\n        results_dict['Model Name'].append(model_name)\n        results_dict['Embedding type'].append(embedding_vector)\n        results_dict['Training Accuracy'].append(train_acc)\n        results_dict['Testing Accuracy'].append(test_acc)\n\n        if test_acc > highest_test_accuracy:\n            highest_test_accuracy = test_acc\n            champion_model_name = model_name\n            champion_model = model\n            champion_embedding = embedding_vector\n\n# Display all accuracies\nresults_df = pd.DataFrame(results_dict)\nprint(\"All Model Accuracies:\\n\")\nprint(results_df.to_string(index=False))\n\n# Print the champion model info\nprint(\"\\nChampion Model:\")\nprint(f\"Name      : {champion_model_name}\")\nprint(f\"Embedding : {champion_embedding}\")\nprint(f\"Accuracy  : {highest_test_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:42:22.272927Z","iopub.execute_input":"2025-04-30T16:42:22.273606Z","iopub.status.idle":"2025-04-30T16:42:33.192675Z","shell.execute_reply.started":"2025-04-30T16:42:22.273582Z","shell.execute_reply":"2025-04-30T16:42:33.192062Z"}},"outputs":[{"name":"stdout","text":"All Model Accuracies:\n\n        Model Name Embedding type  Training Accuracy  Testing Accuracy\n               SVC  TF_IDF 1_gram           0.996357          0.485437\n               SVC  TF_IDF 2_gram           0.986642          0.356796\n        KNeighbors  TF_IDF 1_gram           0.343048          0.349515\n        KNeighbors  TF_IDF 2_gram           0.961141          0.344660\n               XGB  TF_IDF 1_gram           0.852459          0.480583\n               XGB  TF_IDF 2_gram           0.483303          0.354369\n      RandomForest  TF_IDF 1_gram           0.999393          0.456311\n      RandomForest  TF_IDF 2_gram           0.987250          0.378641\n      DecisionTree  TF_IDF 1_gram           0.999393          0.439320\n      DecisionTree  TF_IDF 2_gram           0.987250          0.351942\nLogisticRegression  TF_IDF 1_gram           0.962963          0.507282\nLogisticRegression  TF_IDF 2_gram           0.986642          0.383495\n     MultinomialNB  TF_IDF 1_gram           0.938676          0.495146\n     MultinomialNB  TF_IDF 2_gram           0.986035          0.398058\n\nChampion Model:\nName      : LogisticRegression\nEmbedding : TF_IDF 1_gram\nAccuracy  : 0.5073\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer =AutoTokenizer.from_pretrained('UBC-NLP/MARBERT')\nmodel = AutoModelForSequenceClassification.from_pretrained('UBC-NLP/MARBERT', num_labels=3)\n#-----------------------------------\n# Tokenize the sentences using bert tokenizer\ndf[\"bert_tokens\"] = df.tweet.apply(lambda x: tokenizer(x).tokens())\ndf[\"bert_tokens_ids\"] = df.tweet.apply(lambda x: tokenizer(x).tokens())\ndf[\"encoded\"] = df.tweet.apply(lambda x: tokenizer.encode_plus(x,return_tensors='pt')['input_ids'])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T19:24:52.295154Z","iopub.execute_input":"2025-04-30T19:24:52.295933Z","iopub.status.idle":"2025-04-30T19:24:55.358864Z","shell.execute_reply.started":"2025-04-30T19:24:52.295907Z","shell.execute_reply":"2025-04-30T19:24:55.358100Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"                                                  tweet  class  \\\n0     علمتني الحياه ان الذين يعيشون على الارض ليسوا ...      2   \n1                         ميري كرسمس كل سنة وانتم طيبين      2   \n2                                 و انتهى مشوار الخواجة      0   \n3                             مش عارف ابتدى مذاكره منين      0   \n4     اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...      0   \n...                                                 ...    ...   \n2054  الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...      1   \n2055  نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...      1   \n2056  ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...      2   \n2057                             انت متناقض جدا يا صلاح      0   \n2058     منطقة السيدة زينب ليلة المولد مسجد السيدة زينب      1   \n\n                                            bert_tokens  \\\n0     [[CLS], علمتني, الحياه, ان, الذين, يعيشون, على...   \n1     [[CLS], ميري, كرس, ##مس, كل, سنة, وانتم, طيبين...   \n2              [[CLS], و, انتهى, مشوار, الخواجة, [SEP]]   \n3         [[CLS], مش, عارف, ابتدى, مذاكره, منين, [SEP]]   \n4     [[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...   \n...                                                 ...   \n2054  [[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...   \n2055  [[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...   \n2056  [[CLS], ولد, الهدى, فالك, ##اينات, ضياء, وف, #...   \n2057         [[CLS], انت, متناقض, جدا, يا, صلاح, [SEP]]   \n2058  [[CLS], منطقة, السيدة, زينب, ليلة, المولد, مسج...   \n\n                                        bert_tokens_ids  \\\n0     [[CLS], علمتني, الحياه, ان, الذين, يعيشون, على...   \n1     [[CLS], ميري, كرس, ##مس, كل, سنة, وانتم, طيبين...   \n2              [[CLS], و, انتهى, مشوار, الخواجة, [SEP]]   \n3         [[CLS], مش, عارف, ابتدى, مذاكره, منين, [SEP]]   \n4     [[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...   \n...                                                 ...   \n2054  [[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...   \n2055  [[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...   \n2056  [[CLS], ولد, الهدى, فالك, ##اينات, ضياء, وف, #...   \n2057         [[CLS], انت, متناقض, جدا, يا, صلاح, [SEP]]   \n2058  [[CLS], منطقة, السيدة, زينب, ليلة, المولد, مسج...   \n\n                                                encoded  \n0     [[tensor(2), tensor(8244), tensor(3946), tenso...  \n1     [[tensor(2), tensor(53902), tensor(35685), ten...  \n2     [[tensor(2), tensor(144), tensor(7609), tensor...  \n3     [[tensor(2), tensor(2093), tensor(3323), tenso...  \n4     [[tensor(2), tensor(22181), tensor(1958), tens...  \n...                                                 ...  \n2054  [[tensor(2), tensor(4770), tensor(68899), tens...  \n2055  [[tensor(2), tensor(39939), tensor(3715), tens...  \n2056  [[tensor(2), tensor(3735), tensor(4880), tenso...  \n2057  [[tensor(2), tensor(2030), tensor(27008), tens...  \n2058  [[tensor(2), tensor(5627), tensor(16281), tens...  \n\n[2059 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n      <th>bert_tokens</th>\n      <th>bert_tokens_ids</th>\n      <th>encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>علمتني الحياه ان الذين يعيشون على الارض ليسوا ...</td>\n      <td>2</td>\n      <td>[[CLS], علمتني, الحياه, ان, الذين, يعيشون, على...</td>\n      <td>[[CLS], علمتني, الحياه, ان, الذين, يعيشون, على...</td>\n      <td>[[tensor(2), tensor(8244), tensor(3946), tenso...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ميري كرسمس كل سنة وانتم طيبين</td>\n      <td>2</td>\n      <td>[[CLS], ميري, كرس, ##مس, كل, سنة, وانتم, طيبين...</td>\n      <td>[[CLS], ميري, كرس, ##مس, كل, سنة, وانتم, طيبين...</td>\n      <td>[[tensor(2), tensor(53902), tensor(35685), ten...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>و انتهى مشوار الخواجة</td>\n      <td>0</td>\n      <td>[[CLS], و, انتهى, مشوار, الخواجة, [SEP]]</td>\n      <td>[[CLS], و, انتهى, مشوار, الخواجة, [SEP]]</td>\n      <td>[[tensor(2), tensor(144), tensor(7609), tensor...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>مش عارف ابتدى مذاكره منين</td>\n      <td>0</td>\n      <td>[[CLS], مش, عارف, ابتدى, مذاكره, منين, [SEP]]</td>\n      <td>[[CLS], مش, عارف, ابتدى, مذاكره, منين, [SEP]]</td>\n      <td>[[tensor(2), tensor(2093), tensor(3323), tenso...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...</td>\n      <td>0</td>\n      <td>[[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...</td>\n      <td>[[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...</td>\n      <td>[[tensor(2), tensor(22181), tensor(1958), tens...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...</td>\n      <td>1</td>\n      <td>[[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...</td>\n      <td>[[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...</td>\n      <td>[[tensor(2), tensor(4770), tensor(68899), tens...</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...</td>\n      <td>1</td>\n      <td>[[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...</td>\n      <td>[[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...</td>\n      <td>[[tensor(2), tensor(39939), tensor(3715), tens...</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...</td>\n      <td>2</td>\n      <td>[[CLS], ولد, الهدى, فالك, ##اينات, ضياء, وف, #...</td>\n      <td>[[CLS], ولد, الهدى, فالك, ##اينات, ضياء, وف, #...</td>\n      <td>[[tensor(2), tensor(3735), tensor(4880), tenso...</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>انت متناقض جدا يا صلاح</td>\n      <td>0</td>\n      <td>[[CLS], انت, متناقض, جدا, يا, صلاح, [SEP]]</td>\n      <td>[[CLS], انت, متناقض, جدا, يا, صلاح, [SEP]]</td>\n      <td>[[tensor(2), tensor(2030), tensor(27008), tens...</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>منطقة السيدة زينب ليلة المولد مسجد السيدة زينب</td>\n      <td>1</td>\n      <td>[[CLS], منطقة, السيدة, زينب, ليلة, المولد, مسج...</td>\n      <td>[[CLS], منطقة, السيدة, زينب, ليلة, المولد, مسج...</td>\n      <td>[[tensor(2), tensor(5627), tensor(16281), tens...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# Number of training epochs\nepochs = 20\n# Select the max sentance lenth\nMAX_LEN = 80\n# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size =64","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T19:25:28.894593Z","iopub.execute_input":"2025-04-30T19:25:28.895089Z","iopub.status.idle":"2025-04-30T19:25:28.898706Z","shell.execute_reply.started":"2025-04-30T19:25:28.895065Z","shell.execute_reply":"2025-04-30T19:25:28.897921Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"!pip install keras-preprocessing","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras_preprocessing.sequence import pad_sequences\n\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in df['bert_tokens']]\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T19:25:40.451303Z","iopub.execute_input":"2025-04-30T19:25:40.452131Z","iopub.status.idle":"2025-04-30T19:25:40.480662Z","shell.execute_reply.started":"2025-04-30T19:25:40.452103Z","shell.execute_reply":"2025-04-30T19:25:40.479894Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"attention_masks = []\nfor seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks.append(seq_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T19:25:52.050403Z","iopub.execute_input":"2025-04-30T19:25:52.051140Z","iopub.status.idle":"2025-04-30T19:25:52.115078Z","shell.execute_reply.started":"2025-04-30T19:25:52.051115Z","shell.execute_reply":"2025-04-30T19:25:52.114445Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# Use train_test_split to split our data into train and validation sets for training\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, encoded_labels,\n                                                             random_state=seed, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n                                                             random_state=seed, test_size=0.1)\n\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_dataloader = DataLoader(validation_data, batch_size=batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T19:26:03.159244Z","iopub.execute_input":"2025-04-30T19:26:03.159814Z","iopub.status.idle":"2025-04-30T19:26:03.197149Z","shell.execute_reply.started":"2025-04-30T19:26:03.159790Z","shell.execute_reply":"2025-04-30T19:26:03.196416Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"import torch.optim as optim\n\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay_rate': 0.01},\n                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]\n\n# This variable contains all of the hyperparemeter information our training loop needs\noptimizer = optim.AdamW(optimizer_grouped_parameters,lr=.000005)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T19:26:15.800269Z","iopub.execute_input":"2025-04-30T19:26:15.800554Z","iopub.status.idle":"2025-04-30T19:26:15.808782Z","shell.execute_reply.started":"2025-04-30T19:26:15.800535Z","shell.execute_reply":"2025-04-30T19:26:15.807977Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n#---------------------------------------------\nfrom tqdm import trange\nimport numpy as np\n\nt = []\n# Store our loss and accuracy for plotting\ntrain_loss_set = []\n\nif torch.cuda.is_available():\n    # Transfer the model to GPU\n    model.to(\"cuda\")\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch\"):\n\n  # Training\n  # Set our model to training mode (as opposed to evaluation mode)\n  model.train()\n  # Tracking variables\n  tr_loss = 0\n  nb_tr_examples, nb_tr_steps = 0, 0\n  # Train the data for one epoch\n  for step, batch in enumerate(train_dataloader):\n    # Add batch to GPU\n    b_input_ids, b_input_mask, b_labels = batch\n    b_labels = b_labels.type(torch.LongTensor)   # casting to long\n    # Clear out the gradients (by default they accumulate)\n    optimizer.zero_grad()\n    # Forward pass\n    if torch.cuda.is_available():\n        loss = model(b_input_ids.to(\"cuda\"), token_type_ids=None, attention_mask=b_input_mask.to(\"cuda\"), labels=b_labels.to(\"cuda\"))[\"loss\"]\n    else:\n        loss = model(b_input_ids.to(\"cpu\"), token_type_ids=None, attention_mask=b_input_mask.to(\"cpu\"), labels=b_labels.to(\"cpu\"))[\"loss\"]\n\n    train_loss_set.append(loss.item())\n    # Backward pass\n    loss.backward()\n    # Update parameters and take a step using the computed gradient\n    optimizer.step()\n\n    # Update tracking variables\n    tr_loss += loss.item()\n    nb_tr_examples += b_input_ids.size(0)\n    nb_tr_steps += 1\n\n  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n  # Validation\n  # Put model in evaluation mode to evaluate loss on the validation set\n  model.eval()\n  # Tracking variables\n  eval_loss, eval_accuracy = 0, 0\n  nb_eval_steps, nb_eval_examples = 0, 0\n\n  # Evaluate data for one epoch\n  for batch in validation_dataloader:\n    # Add batch to GPU\n    # batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    b_labels = b_labels.type(torch.LongTensor)   # casting to long\n    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n    with torch.no_grad():\n      # Forward pass, calculate logit predictions\n        if torch.cuda.is_available():\n            logits = model(b_input_ids.to(\"cuda\"), token_type_ids=None, attention_mask=b_input_mask.to(\"cuda\"))\n        else:\n            logits = model(b_input_ids.to(\"cpu\"), token_type_ids=None, attention_mask=b_input_mask.to(\"cpu\"))\n\n    # Move logits and labels to CPU\n    logits = logits[\"logits\"].detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    eval_accuracy += tmp_eval_accuracy\n    nb_eval_steps += 1\n\n  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n  if (eval_accuracy/nb_eval_steps) > 0.78:\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T19:26:51.208577Z","iopub.execute_input":"2025-04-30T19:26:51.208863Z","iopub.status.idle":"2025-04-30T19:28:58.300738Z","shell.execute_reply.started":"2025-04-30T19:26:51.208836Z","shell.execute_reply":"2025-04-30T19:28:58.300047Z"}},"outputs":[{"name":"stderr","text":"Epoch:   0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0825892851270478\n","output_type":"stream"},{"name":"stderr","text":"Epoch:   5%|▌         | 1/20 [00:25<08:06, 25.63s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.47265625\nTrain loss: 0.9461242186612097\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  10%|█         | 2/20 [00:51<07:46, 25.92s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7003348214285714\nTrain loss: 0.7086904788839405\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  15%|█▌        | 3/20 [01:16<07:08, 25.21s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7276785714285714\nTrain loss: 0.5101674651277477\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  20%|██        | 4/20 [01:41<06:42, 25.14s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7544642857142857\nTrain loss: 0.3787607478684393\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  20%|██        | 4/20 [02:06<08:27, 31.72s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7979910714285714\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"df_submit = pd.read_csv(\"/kaggle/input/nlp-arabic-tweets-debi-intake-2/test.csv\")\ndf_submit[\"tweet\"] = df_submit.tweet.apply(lambda x: data_cleaning(x))\ndf_submit['tweet']=df_submit['tweet'].apply(lambda x: arabert_prep.preprocess(x))\n# Tokenize the sentences using bert tokenizer\ndf_submit[\"bert_tokens\"] = df_submit.tweet.apply(lambda x: tokenizer(x).tokens())\n#---------------------------------\ninput_ids_submit = [tokenizer.convert_tokens_to_ids(x) for x in df_submit[\"bert_tokens\"]]\ninput_ids_submit = pad_sequences(input_ids_submit, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\nattention_masks_submit = []\nfor seq in input_ids_submit:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks_submit.append(seq_mask)\n    \ninputs_submit = torch.tensor(input_ids_submit)\nmasks_submit = torch.tensor(attention_masks_submit)\nsubmit_data = TensorDataset(inputs_submit, masks_submit)\nsubmit_dataloader = DataLoader(submit_data, batch_size=batch_size)\nmodel.eval()\nif torch.cuda.is_available():\n    model.to(\"cuda\")\n\noutputs = []\nfor input, masks in submit_dataloader:\n    torch.cuda.empty_cache() # empty the gpu memory\n    if torch.cuda.is_available():\n        # Transfer the batch to gpu\n        input = input.to('cuda')\n        masks = masks.to('cuda')\n    # Run inference on the batch\n    output = model(input, attention_mask=masks)[\"logits\"]\n    # Transfer the output to CPU again and convert to numpy\n    output = output.cpu().detach().numpy()\n    # Store the output in a list\n    outputs.append(output)\n# Concatenate all the lists within the list into one list\noutputs = [x for y in outputs for x in y]\n# Inverse transform the label encoding\npred_flat = np.argmax(outputs, axis=1).flatten()\noutput_labels = lable_encoder.inverse_transform(pred_flat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T19:28:58.302139Z","iopub.execute_input":"2025-04-30T19:28:58.302445Z","iopub.status.idle":"2025-04-30T19:29:04.430334Z","shell.execute_reply.started":"2025-04-30T19:28:58.302420Z","shell.execute_reply":"2025-04-30T19:29:04.429765Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"submission = pd.DataFrame({\"Id\":np.arange(1, len(output_labels)+1), \"class\":output_labels})\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T19:29:51.446009Z","iopub.execute_input":"2025-04-30T19:29:51.446340Z","iopub.status.idle":"2025-04-30T19:29:51.452561Z","shell.execute_reply.started":"2025-04-30T19:29:51.446316Z","shell.execute_reply":"2025-04-30T19:29:51.451941Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}